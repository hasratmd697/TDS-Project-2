# TDS Project 2 - LLM-based Quiz Solver

This project implements an automated system to solve dynamic quizzes using an LLM (Large Language Model). It consists of a FastAPI server that receives a task URL, generates a Python script using the AIPIPE API to solve the specific quiz on that page, and then executes the script to submit the answer.

## Project Structure

- **`main.py`**: The main FastAPI application. It exposes an endpoint `/receive_request` that:
  1.  Accepts a JSON payload with user details and a target URL.
  2.  Prompts an LLM (via AIPIPE) to generate a Python script tailored to the quiz found at the URL.
  3.  Saves the generated code to `generated_script.py`.
  4.  Executes `generated_script.py` to solve the quiz and submit the answer.
- **`test_request.py`**: A client script to send a test request to the deployed (or local) FastAPI server.
- **`send_request.py`**: A client script to send a test request to the local FastAPI server.
- **`generated_script.py`**: The script dynamically generated by the LLM to solve the current task.
- **`requirements.txt`**: List of Python dependencies.

## Setup

1.  **Clone the repository** (if applicable).

2.  **Install Dependencies**:
    Ensure you have Python installed, then run:

    ```bash
    pip install -r requirements.txt
    ```

3.  **Environment Configuration**:
    Create a `.env` file in the root directory with the following keys (see `envexample.txt`):
    ```ini
    AIPIPE_TOKEN=your_aipipe_token_here
    AIPIPE_API_URL=https://aipipe.org/openrouter/v1/chat/completions
    SECRET=your_secret_key_here
    ```

## Usage

1.  **Start the Server Locally**:
    Run the FastAPI server using `uvicorn` (or via the script if configured):

    ```bash
    uv run main.py
    # OR directly with python if not using uv
    python main.py
    ```

    The server will start at `http://localhost:8000`.

2.  **Send a Request**:
    You can use `test_request.py` to test either the local or deployed version.

    ```bash
    uv run test_request.py
    # OR
    python test_request.py
    ```

    Follow the prompts to enter your URL (e.g., `http://localhost:8000` or your Vercel URL).

## Deployment

The application is ready to be deployed on Vercel.

1.  Install Vercel CLI: `npm i -g vercel`
2.  Run `vercel` in the project directory.
3.  Set the environment variables (`AIPIPE_TOKEN`, `AIPIPE_API_URL`, `SECRET`) in the Vercel dashboard.

## Requirements

- Python 3.11+
- `httpx`
- `fastapi`
- `uvicorn`
- `python-dotenv`
- `beautifulsoup4`
